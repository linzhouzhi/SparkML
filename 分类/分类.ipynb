{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   从数据中抽取合适的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(\"http://www.bloomberg.com/news/2010-12-23/ibm-predicts-holographic-calls-air-breathing-batteries-by-2015.html\", \"4042\", \"{\"\"title\"\":\"\"IBM Sees Holographic Calls Air Breathing Batteries ibm sees holographic calls, air-breathing batteries\"\",\"\"body\"\":\"\"A sign stands outside the International Business Machines Corp IBM Almaden Research Center campus in San Jose California Photographer Tony Avelar Bloomberg Buildings stand at the International Business Machines Corp IBM Almaden Research Center campus in the Santa Teresa Hills of San Jose California Photographer Tony Avelar Bloomberg By 2015 your mobile phone will project a 3 D image of anyone who calls and your laptop will be powered by kinetic energy At least that s what International Business Machines Corp sees ..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawData = sc.textFile(\"../data/kaggle/train_noheader.tsv\")\n",
    "val records = rawData.map(line => line.split(\"\\t\"))\n",
    "records.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "val data = records.map{\n",
    "    r =>\n",
    "    val trimmed = r.map(_.replaceAll(\"\\\"\",\"\"))\n",
    "    val label = trimmed(r.size -1).toInt\n",
    "    val features = trimmed.slice(4,r.size-1).map(d => if(d==\"?\") 0.0 else d.toDouble)\n",
    "    LabeledPoint(label, Vectors.dense(features))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7395"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cache\n",
    "val numData = data.count\n",
    "numData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val nbData = records.map{\n",
    "    r =>\n",
    "    val trimmed = r.map(_.replaceAll(\"\\\"\",\"\"))\n",
    "    val label = trimmed(r.size -1).toInt\n",
    "    val features = trimmed.slice(4,r.size-1).map(d => if(d==\"?\") 0.0 else d.toDouble).map(d => if (d<0) 0.0 else d )\n",
    "    LabeledPoint(label, Vectors.dense(features))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.classification.LogisticRegressionWithSGD\n",
    "import org.apache.spark.mllib.classification.SVMWithSGD\n",
    "import org.apache.spark.mllib.classification.NaiveBayes\n",
    "import org.apache.spark.mllib.tree.DecisionTree\n",
    "import org.apache.spark.mllib.tree.configuration.Algo\n",
    "import org.apache.spark.mllib.tree.impurity.Entropy\n",
    "val numIterations = 10\n",
    "val maxTreeDepth = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrModel = LogisticRegressionWithSGD.train(data, numIterations)\n",
    "lrModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val svmModel = SVMWithSGD.train(data, numIterations)\n",
    "svmModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.classification.NaiveBayesModel@72b0e188"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nbModel = NaiveBayes.train(nbData)\n",
    "nbModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeModel classifier of depth 5 with 61 nodes"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dtModel = DecisionTree.train(data, Algo.Classification, Entropy, maxTreeDepth)\n",
    "dtModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataPoint = data.first\n",
    "val prediction = lrModel.predict(dataPoint.features)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trueLabel = dataPoint.label\n",
    "trueLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1.0, 1.0, 1.0, 1.0, 1.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = lrModel.predict(data.map(lp => lp.features))\n",
    "predictions.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估分类模型的性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测正确率和错误率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5146720757268425"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrTotalCorrect = data.map{ point =>\n",
    "    if ( lrModel.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "val lrAccuracy = lrTotalCorrect / data.count\n",
    "lrAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val svmTotalCorrect = data.map{point=>\n",
    "    if(svmModel.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "val nbTotalCorrect = nbData.map{ point =>\n",
    "    if(nbModel.predict(point.features) == point.label) 1 else 0\n",
    "}.sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val dtTotalCorrect = data.map{ point =>\n",
    "    val score = dtModel.predict(point.features)\n",
    "    val predicted = if(score > 0.5) 1 else 0\n",
    "    if( predicted == point.label ) 1 else 0\n",
    "}.sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5146720757268425"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val svmAccuracy = svmTotalCorrect / numData\n",
    "svmAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5803921568627451"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nbAccuracy = nbTotalCorrect / numData\n",
    "nbAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6482758620689655"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dtAccuracy = dtTotalCorrect / numData\n",
    "dtAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准确率和召回率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC曲线和AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "val metrics = Seq(lrModel,svmModel).map{ model =>\n",
    "    val scoreAndLabels = data.map{ point =>\n",
    "        (model.predict(point.features),point.label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (model.getClass.getSimpleName,metrics.areaUnderPR,metrics.areaUnderROC)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val nbMetrics = Seq(nbModel).map{ model =>\n",
    "    val scoreAndLabels = nbData.map{ point =>\n",
    "        val score = model.predict(point.features)\n",
    "        (if (score > 0.5) 1.0 else 0.0, point.label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (model.getClass.getSimpleName, metrics.areaUnderPR,metrics.areaUnderROC)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val dtMetrics = Seq(dtModel).map{ model =>\n",
    "    val scoreAndLabels = data.map{ point =>\n",
    "        val score = model.predict(point.features)\n",
    "        (if (score > 0.5) 1.0 else 0.0, point.label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (model.getClass.getSimpleName,metrics.areaUnderPR,metrics.areaUnderROC)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel, Area under PR: 75.6759%, Area under ROC: 50.1418%\n",
      "SVMModel, Area under PR: 75.6759%, Area under ROC: 50.1418%\n",
      "NaiveBayesModel, Area under PR: 68.0851%, Area under ROC: 58.3559%\n",
      "DecisionTreeModel, Area under PR: 74.3081%, Area under ROC: 64.8837%\n"
     ]
    }
   ],
   "source": [
    "val allMetrics = metrics ++ nbMetrics ++ dtMetrics\n",
    "allMetrics.foreach{ case (m, pr, roc) =>\n",
    "    println(f\"$m, Area under PR: ${pr * 100.0}%2.4f%%, Area under ROC: ${roc * 100.0}%2.4f%%\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改进模型性能以及参数调优"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4122580529952672,2.761823191986608,0.4682304732861389,0.21407992638350232,0.09206236071899916,0.04926216043908053,2.255103452212041,-0.10375042752143335,0.0,0.05642274498417851,0.02123056118999324,0.23377817665490194,0.2757090373659236,0.615551048005409,0.6603110209601082,30.07707910750513,0.03975659229208925,5716.598242055447,178.75456389452353,4.960649087221096,0.17286405047031742,0.10122079189276552]\n"
     ]
    }
   ],
   "source": [
    " import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
    " val vectors = data.map( lp => lp.features )\n",
    " val matrix = new RowMatrix(vectors)\n",
    " val matrixSummary = matrix.computeColumnSummaryStatistics()\n",
    " println(matrixSummary.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,-1.0,0.0,0.0,0.0,0.045564223,-1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0]\n"
     ]
    }
   ],
   "source": [
    "println(matrixSummary.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.999426,363.0,1.0,1.0,0.980392157,0.980392157,21.0,0.25,0.0,0.444444444,1.0,0.716883117,113.3333333,1.0,1.0,100.0,1.0,207952.0,4997.0,22.0,1.0,1.0]\n"
     ]
    }
   ],
   "source": [
    "println(matrixSummary.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10974244167559001,74.30082476809639,0.04126316989120241,0.02153343633200108,0.009211817450882448,0.005274933469767946,32.53918714591821,0.09396988697611545,0.0,0.0017177410346628928,0.020782634824610638,0.0027548394224293036,3.683788919674426,0.2366799607085986,0.22433071201674218,415.8785589543846,0.03818116876739597,7.877330081138463E7,32208.116247426184,10.45300904576431,0.03359363403832393,0.006277532884214705]\n"
     ]
    }
   ],
   "source": [
    "println(matrixSummary.variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5053.0,7354.0,7172.0,6821.0,6160.0,5128.0,7350.0,1257.0,0.0,7362.0,157.0,7395.0,7355.0,4552.0,4883.0,7347.0,294.0,7378.0,7395.0,6782.0,6868.0,7235.0]\n"
     ]
    }
   ],
   "source": [
    "println(matrixSummary.numNonzeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.feature.StandardScaler\n",
    "val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)\n",
    "val scaledData = data.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575]\n"
     ]
    }
   ],
   "source": [
    "println( data.first.features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.137647336497678,-0.08193557169294771,1.0251398128933331,-0.05586356442541689,-0.4688932531289357,-0.3543053263079386,-0.3175352172363148,0.3384507982396541,0.0,0.828822173315322,-0.14726894334628504,0.22963982357813484,-0.14162596909880876,0.7902380499177364,0.7171947294529865,-0.29799681649642257,-0.2034625779299476,-0.03296720969690391,-0.04878112975579913,0.9400699751165439,-0.10869848852526258,-0.2788207823137022]\n"
     ]
    }
   ],
   "source": [
    "println( scaledData.first.features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.137647336497678\n"
     ]
    }
   ],
   "source": [
    "println( ( 0.789131 - 0.4122580529952672 ) / math.sqrt( 0.10974244167559001 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel\n",
      "Accuracy:62.0419%\n",
      "Area under PR: 72.7254%\n",
      "Area under ROC: 61.9663%\n"
     ]
    }
   ],
   "source": [
    "val lrModelScaled = LogisticRegressionWithSGD.train( scaledData, numIterations )\n",
    "val lrTotalCorrectScaled = scaledData.map { point =>\n",
    "    if (lrModelScaled.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "val lrAccuracyScaled = lrTotalCorrectScaled / numData\n",
    "val lrPredictionsVsTrue = scaledData.map{ point =>\n",
    "    (lrModelScaled.predict(point.features),point.label)\n",
    "}\n",
    "val lrMetricsScaled = new BinaryClassificationMetrics(lrPredictionsVsTrue)\n",
    "val lrPr = lrMetricsScaled.areaUnderPR\n",
    "val lrRoc = lrMetricsScaled.areaUnderROC\n",
    "println(f\"${lrModelScaled.getClass.getSimpleName}\\nAccuracy:${lrAccuracyScaled * 100}%2.4f%%\\nArea under PR: ${lrPr * 100.0}%2.4f%%\\nArea under ROC: ${lrRoc * 100.0}%2.4f%%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其它特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map(\"weather\" -> 0, \"sports\" -> 6, \"unknown\" -> 4, \"computer_internet\" -> 12, \"?\" -> 11, \"culture_politics\" -> 3, \"religion\" -> 8, \"recreation\" -> 2, \"arts_entertainment\" -> 9, \"health\" -> 5, \"law_crime\" -> 10, \"gaming\" -> 13, \"business\" -> 1, \"science_technology\" -> 7)\n"
     ]
    }
   ],
   "source": [
    "val categories = records.map(r => r(3)).distinct.collect.zipWithIndex.toMap\n",
    "val numCategories = categories.size\n",
    "println(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "println(numCategories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0,[0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575])\n"
     ]
    }
   ],
   "source": [
    "val dataCategories = records.map{ r =>\n",
    "    val trimmed = r.map(_.replaceAll(\"\\\"\",\"\"))\n",
    "    val label = trimmed(r.size -1).toInt\n",
    "    val categoryIdx = categories(r(3))\n",
    "    val categoryFeatures = Array.ofDim[Double](numCategories)\n",
    "    categoryFeatures(categoryIdx) = 1.0\n",
    "    val otherFeatures = trimmed.slice(4,r.size -1).map( d => if(d == \"?\") 0.0 else d.toDouble)\n",
    "    val features = categoryFeatures ++ otherFeatures\n",
    "    LabeledPoint( label, Vectors.dense(features))\n",
    "}\n",
    "println( dataCategories.first )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val scalerCats = new StandardScaler(withMean = true, withStd = true).fit(dataCategories.map(lp => lp.features))\n",
    "val scaledDataCats = dataCategories.map(lp => LabeledPoint(lp.label,scalerCats.transform(lp.features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575]\n"
     ]
    }
   ],
   "source": [
    "println( dataCategories.first.features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02326210589837061,2.7207366564548514,-0.4464212047941535,-0.22052688457880879,-0.028494000387023734,-0.2709990696925828,-0.23272797709480803,-0.2016540523193296,-0.09914991930875496,-0.38181322324318134,-0.06487757239262681,-0.6807527904251456,-0.20418221057887365,-0.10189469097220732,1.137647336497678,-0.08193557169294771,1.0251398128933331,-0.05586356442541689,-0.4688932531289357,-0.3543053263079386,-0.3175352172363148,0.3384507982396541,0.0,0.828822173315322,-0.14726894334628504,0.22963982357813484,-0.14162596909880876,0.7902380499177364,0.7171947294529865,-0.29799681649642257,-0.2034625779299476,-0.03296720969690391,-0.04878112975579913,0.9400699751165439,-0.10869848852526258,-0.2788207823137022]\n"
     ]
    }
   ],
   "source": [
    "println( scaledDataCats.first.features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel\n",
      "Accuracy:66.5720%\n",
      "Area under PR: 75.7964%\n",
      "Area under ROC: 66.5483%\n"
     ]
    }
   ],
   "source": [
    "val lrModelScaledCats = LogisticRegressionWithSGD.train( scaledDataCats, numIterations )\n",
    "val lrTotalCorrectScaledCats = scaledDataCats.map { point =>\n",
    "    if (lrModelScaledCats.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "val lrAccuracyScaledCats = lrTotalCorrectScaledCats / numData\n",
    "val lrPredictionsVsTrueCats = scaledDataCats.map{ point =>\n",
    "    (lrModelScaledCats.predict(point.features),point.label)\n",
    "}\n",
    "val lrMetricsScaledCats = new BinaryClassificationMetrics(lrPredictionsVsTrueCats)\n",
    "val lrPrCats = lrMetricsScaledCats.areaUnderPR\n",
    "val lrRocCats = lrMetricsScaledCats.areaUnderROC\n",
    "println(f\"${lrModelScaledCats.getClass.getSimpleName}\\nAccuracy:${lrAccuracyScaledCats * 100}%2.4f%%\\nArea under PR: ${lrPrCats * 100.0}%2.4f%%\\nArea under ROC: ${lrRocCats * 100.0}%2.4f%%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val dataNB = records.map{ r =>\n",
    "    val trimmed = r.map(_.replaceAll(\"\\\"\",\"\"))\n",
    "    val label = trimmed( r.size - 1 ).toInt\n",
    "    val categoryIdx = categories( r(3) )\n",
    "    val categoryFeatures = Array.ofDim[Double](numCategories)\n",
    "    categoryFeatures(categoryIdx) = 1.0\n",
    "    LabeledPoint( label, Vectors.dense(categoryFeatures))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayesModel\n",
      "Accuracy:60.9601%\n",
      "Area under PR: 74.0522%\n",
      "Area under ROC: 60.5138%\n"
     ]
    }
   ],
   "source": [
    "val nbModelCats = NaiveBayes.train(dataNB)\n",
    "val nbTotalCorrectCats = dataNB.map{ point =>\n",
    "    if( nbModelCats.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "val nbAccuracyCats = nbTotalCorrectCats / numData\n",
    "val nbPredictionsVsTrueCats = dataNB.map{ point =>\n",
    "    (nbModelCats.predict(point.features), point.label)\n",
    "}\n",
    "val nbMetricsCats = new BinaryClassificationMetrics(nbPredictionsVsTrueCats)\n",
    "val nbPrCats = nbMetricsCats.areaUnderPR\n",
    "val nbRocCats = nbMetricsCats.areaUnderROC\n",
    "println(f\"${nbModelCats.getClass.getSimpleName}\\nAccuracy:${nbAccuracyCats * 100}%2.4f%%\\nArea under PR: ${nbPrCats * 100.0}%2.4f%%\\nArea under ROC: ${nbRocCats * 100.0}%2.4f%%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型参数调优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.optimization.Updater\n",
    "import org.apache.spark.mllib.optimization.SimpleUpdater\n",
    "import org.apache.spark.mllib.optimization.L1Updater\n",
    "import org.apache.spark.mllib.optimization.SquaredL2Updater\n",
    "import org.apache.spark.mllib.classification.ClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainWithParams( input: RDD[LabeledPoint], regParam: Double, numIterations: Int, updater: Updater, stepSize: Double) = {\n",
    "    val lr = new LogisticRegressionWithSGD\n",
    "    lr.optimizer.setNumIterations(numIterations).setUpdater(updater).setRegParam(regParam).setStepSize(stepSize)\n",
    "    lr.run(input)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createMetrics(label: String, data: RDD[LabeledPoint],model:ClassificationModel) = {\n",
    "    val scoreAndLabels = data.map{ point =>\n",
    "        (model.predict(point.features), point.label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics( scoreAndLabels )\n",
    "    ( label, metrics.areaUnderROC)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[342] at map at <console>:64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaledDataCats.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iterations, AUC = 64.95%\n",
      "5 iterations, AUC = 66.62%\n",
      "10 iterations, AUC = 66.55%\n",
      "50 iterations, AUC = 66.81%\n"
     ]
    }
   ],
   "source": [
    "val iterResults = Seq(1 ,5 ,10, 50).map { param =>\n",
    "    val model = trainWithParams(scaledDataCats, 0.0, param, new SimpleUpdater, 1.0)\n",
    "    createMetrics(s\"$param iterations\", scaledDataCats, model)\n",
    "}\n",
    "iterResults.foreach{ case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 step size, AUC = 64.97%\n",
      "0.01 step size, AUC = 64.96%\n",
      "0.1 step size, AUC = 65.52%\n",
      "10.0 step size, AUC = 61.92%\n"
     ]
    }
   ],
   "source": [
    "val stepResults = Seq(0.001, 0.01, 0.1, 10.0).map{ param =>\n",
    "    val model = trainWithParams(scaledDataCats, 0.0, numIterations, new SimpleUpdater, param)\n",
    "    createMetrics(s\"$param step size\", scaledDataCats, model)\n",
    "}\n",
    "stepResults.foreach{ case ( param, auc ) => println( f\"$param, AUC = ${auc * 100}%2.2f%%\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 L2 regularization parameter, AUC=66.55%\n",
      "0.01 L2 regularization parameter, AUC=66.55%\n",
      "0.1 L2 regularization parameter, AUC=66.63%\n",
      "10.0 L2 regularization parameter, AUC=35.33%\n"
     ]
    }
   ],
   "source": [
    "val regResults = Seq(0.001, 0.01, 0.1, 10.0).map{ param =>\n",
    "    val model = trainWithParams(scaledDataCats, param, numIterations,new SquaredL2Updater,1.0)\n",
    "    createMetrics(s\"$param L2 regularization parameter\", scaledDataCats, model)\n",
    "}\n",
    "regResults.foreach{ case (param, auc) => println(f\"$param, AUC=${auc * 100}%2.2f%%\") }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.tree.impurity.Impurity\n",
    "import org.apache.spark.mllib.tree.impurity.Entropy\n",
    "import org.apache.spark.mllib.tree.impurity.Gini\n",
    "\n",
    "def trainDTWithParams(input: RDD[LabeledPoint], maxDepth: Int, impurity: Impurity) = {\n",
    "    DecisionTree.train(input, Algo.Classification, impurity, maxDepth)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tree depth, AUC = 59.33%\n",
      "2 tree depth, AUC = 61.68%\n",
      "3 tree depth, AUC = 62.61%\n",
      "4 tree depth, AUC = 63.63%\n",
      "5 tree depth, AUC = 64.88%\n",
      "10 tree depth, AUC = 76.26%\n",
      "20 tree depth, AUC = 98.45%\n"
     ]
    }
   ],
   "source": [
    "val dtResultsEntropy = Seq(1,2,3,4,5,10,20).map{ param =>\n",
    "    val model = trainDTWithParams(data, param,Entropy)\n",
    "    val scoreAndLabels = data.map{ point =>\n",
    "        val score = model.predict(point.features)\n",
    "        (if (score > 0.5) 1.0 else 0.0, point.label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (s\"$param tree depth\", metrics.areaUnderROC)\n",
    "}\n",
    "dtResultsEntropy.foreach{case (param,auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 lambda, AUC=60.51%\n",
      "0.01 lambda, AUC=60.51%\n",
      "0.1 lambda, AUC=60.51%\n",
      "1.0 lambda, AUC=60.51%\n",
      "10.0 lambda, AUC=60.51%\n"
     ]
    }
   ],
   "source": [
    "def trainNBWithParams(input: RDD[LabeledPoint],lambda: Double) = {\n",
    "    val nb = new NaiveBayes\n",
    "    nb.setLambda(lambda)\n",
    "    nb.run(input)\n",
    "}\n",
    "val nbResults = Seq(0.001,0.01,0.1,1.0,10.0).map{ param =>\n",
    "    val model = trainNBWithParams(dataNB, param)\n",
    "    val scoreAndLabels = dataNB.map{ point =>\n",
    "        (model.predict(point.features), point.label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (s\"$param lambda\", metrics.areaUnderROC)\n",
    "}\n",
    "nbResults.foreach{ case(param, auc) => println(f\"$param, AUC=${auc * 100}%2.2f%%\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val trainTestSplit = scaledDataCats.randomSplit(Array(0.6, 0.4), 123)\n",
    "val train = trainTestSplit(0)\n",
    "val test = trainTestSplit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 L2 regularization parameter, AUC = 67.173110%\n",
      "0.001 L2 regularization parameter, AUC = 67.173110%\n",
      "0.0025 L2 regularization parameter, AUC = 67.173110%\n",
      "0.005 L2 regularization parameter, AUC = 67.140600%\n",
      "0.01 L2 regularization parameter, AUC = 67.175986%\n"
     ]
    }
   ],
   "source": [
    "val regResultsTest = Seq(0.0, 0.001, 0.0025, 0.005, 0.01).map{ param =>\n",
    "    val model = trainWithParams( train, param, numIterations, new SquaredL2Updater, 1.0)\n",
    "    createMetrics(s\"$param L2 regularization parameter\", test, model)\n",
    "}\n",
    "regResultsTest.foreach{ case(param, auc) => println(f\"$param, AUC = ${auc * 100}%2.6f%%\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.5.2 (Scala 2.10)",
   "language": "",
   "name": "spark"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
